// 2023-04-19 18:03
#include <neon.h>
#include <neon_test.h>
// int8x8_t vsra_n_s8(int8x8_t a,int8x8_t b,const int n)
// int16x4_t vsra_n_s16(int16x4_t a,int16x4_t b,const int n)
// int32x2_t vsra_n_s32(int32x2_t a,int32x2_t b,const int n)
// int64x1_t vsra_n_s64(int64x1_t a,int64x1_t b,const int n)
// uint8x8_t vsra_n_u8(uint8x8_t a,uint8x8_t b,const int n)
// uint16x4_t vsra_n_u16(uint16x4_t a,uint16x4_t b,const int n)
// uint32x2_t vsra_n_u32(uint32x2_t a,uint32x2_t b,const int n)
// uint64x1_t vsra_n_u64(uint64x1_t a,uint64x1_t b,const int n)
//
// int8x16_t vsraq_n_s8(int8x16_t a,int8x16_t b,const int n)
// int16x8_t vsraq_n_s16(int16x8_t a,int16x8_t b,const int n)
// int32x4_t vsraq_n_s32(int32x4_t a,int32x4_t b,const int n)
// int64x2_t vsraq_n_s64(int64x2_t a,int64x2_t b,const int n)
// uint8x16_t vsraq_n_u8(uint8x16_t a,uint8x16_t b,const int n)
// uint16x8_t vsraq_n_u16(uint16x8_t a,uint16x8_t b,const int n)
// uint32x4_t vsraq_n_u32(uint32x4_t a,uint32x4_t b,const int n)
// uint64x2_t vsraq_n_u64(uint64x2_t a,uint64x2_t b,const int n)
// -------------------------------------------------------------
// int64_t vsrad_n_s64(int64_t a,int64_t b,const int n)
// uint64_t vsrad_n_u64(uint64_t a,uint64_t b,const int n)

TEST_CASE(test_vsra_n_s8) {
    struct {
        int8_t a[8];
        int8_t b[8];
        int8_t r1[8];
        int8_t r3[8];
        int8_t r5[8];
        int8_t r6[8];
        int8_t r8[8];
    } test_vec[] = {
        {{-110, -96, -68, 99, 115, 79, 90, -125},
         {44, 15, 85, 89, 6, -90, -51, -80},
         {-88, -89, -26, -113, 118, 34, 64, 91},
         {-105, -95, -58, 110, 115, 67, 83, 121},
         {-109, -96, -66, 101, 115, 76, 88, INT8_MIN},
         {-110, -96, -67, 100, 115, 77, 89, -127},
         {-110, -96, -68, 99, 115, 78, 89, -126}},
        {{22, -35, 93, -94, -68, 118, 60, -98},
         {-104, 7, 42, 107, -70, 83, -72, 76},
         {-30, -32, 114, -41, -103, -97, 24, -60},
         {9, -35, 98, -81, -77, INT8_MIN, 51, -89},
         {18, -35, 94, -91, -71, 120, 57, -96},
         {20, -35, 93, -93, -70, 119, 58, -97},
         {21, -35, 93, -94, -69, 118, 59, -98}},
        {{-13, 116, -81, 102, -60, 10, -23, -16},
         {25, 63, 74, 31, -27, 23, -49, -4},
         {-1, -109, -44, 117, -74, 21, -48, -18},
         {-10, 123, -72, 105, -64, 12, -30, -17},
         {-13, 117, -79, 102, -61, 10, -25, -17},
         {-13, 116, -80, 102, -61, 10, -24, -17},
         {-13, 116, -81, 102, -61, 10, -24, -17}},
        {{-12, 44, -98, -80, -93, -38, 79, 59},
         {-31, 121, -90, -100, -52, 95, -24, -64},
         {-28, 104, 113, 126, -119, 9, 67, 27},
         {-16, 59, -110, -93, -100, -27, 76, 51},
         {-13, 47, -101, -84, -95, -36, 78, 57},
         {-13, 45, -100, -82, -94, -37, 78, 58},
         {-13, 44, -99, -81, -94, -38, 78, 58}},
        {{-45, -104, 38, -105, -94, 16, -120, -69},
         {79, -46, -37, 52, -23, -86, 48, -34},
         {-6, -127, 19, -79, -106, -27, -96, -86},
         {-36, -110, 33, -99, -97, 5, -114, -74},
         {-43, -106, 36, -104, -95, 13, -119, -71},
         {-44, -105, 37, -105, -95, 14, -120, -70},
         {-45, -105, 37, -105, -95, 15, -120, -70}},
        {{-41, -49, -114, 122, -87, -35, -75, -117},
         {87, 92, 39, 35, -69, 15, -29, -114},
         {2, -3, -95, -117, -122, -28, -90, 82},
         {-31, -38, -110, 126, -96, -34, -79, 124},
         {-39, -47, -113, 123, -90, -35, -76, -121},
         {-40, -48, -114, 122, -89, -35, -76, -119},
         {-41, -49, -114, 122, -88, -35, -76, -118}},
        {{-89, 10, 38, 73, 26, -82, 5, 105},
         {INT8_MIN, -32, -99, 105, -118, -50, 71, 97},
         {103, -6, -12, 125, -33, -107, 40, -103},
         {-105, 6, 25, 86, 11, -89, 13, 117},
         {-93, 9, 34, 76, 22, -84, 7, 108},
         {-91, 9, 36, 74, 24, -83, 6, 106},
         {-90, 9, 37, 73, 25, -83, 5, 105}},
        {{-99, -42, -37, 70, -77, -111, -47, 10},
         {-19, -8, 46, -88, 8, 17, 54, -81},
         {-109, -46, -14, 26, -73, -103, -20, -31},
         {-102, -43, -32, 59, -76, -109, -41, -1},
         {-100, -43, -36, 67, -77, -111, -46, 7},
         {-100, -43, -37, 68, -77, -111, -47, 8},
         {-100, -43, -37, 69, -77, -111, -47, 9}},
    };

    for (size_t i = 0; i < (sizeof(test_vec) / sizeof(test_vec[0])); i++) {
        int8x8_t a = vld1_s8(test_vec[i].a);
        int8x8_t b = vld1_s8(test_vec[i].b);

        int8x8_t r1 = vsra_n_s8(a, b, 1);
        int8x8_t r3 = vsra_n_s8(a, b, 3);
        int8x8_t r5 = vsra_n_s8(a, b, 5);
        int8x8_t r6 = vsra_n_s8(a, b, 6);
        /* int8x8_t r8 = vsra_n_s8(a, b, 8); */

        int8x8_t check1 = vld1_s8(test_vec[i].r1);
        int8x8_t check3 = vld1_s8(test_vec[i].r3);
        int8x8_t check5 = vld1_s8(test_vec[i].r5);
        int8x8_t check6 = vld1_s8(test_vec[i].r6);
        /* int8x8_t check8 = vld1_s8(test_vec[i].r8); */

        ASSERT_EQUAL(r1, check1);
        ASSERT_EQUAL(r3, check3);
        ASSERT_EQUAL(r5, check5);
        ASSERT_EQUAL(r6, check6);
        /* ASSERT_EQUAL(r8, check8); */
    }

    return 0;
}
