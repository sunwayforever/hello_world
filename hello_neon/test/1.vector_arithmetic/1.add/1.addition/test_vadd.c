// 2023-04-14 10:44
#include <neon.h>
#include <neon_test.h>
// int8x8_t vadd_s8(int8x8_t a,int8x8_t b)
// int16x4_t vadd_s16(int16x4_t a,int16x4_t b)
// int32x2_t vadd_s32(int32x2_t a,int32x2_t b)
// int64x1_t vadd_s64(int64x1_t a,int64x1_t b)
// uint8x8_t vadd_u8(uint8x8_t a,uint8x8_t b)
// uint16x4_t vadd_u16(uint16x4_t a,uint16x4_t b)
// uint32x2_t vadd_u32(uint32x2_t a,uint32x2_t b)
// uint64x1_t vadd_u64(uint64x1_t a,uint64x1_t b)
//
// int8x16_t vaddq_s8(int8x16_t a,int8x16_t b)
//               ^--- q è¡¨ç¤º 128-bit vector
// int16x8_t vaddq_s16(int16x8_t a,int16x8_t b)
// int32x4_t vaddq_s32(int32x4_t a,int32x4_t b)
// int64x2_t vaddq_s64(int64x2_t a,int64x2_t b)
// uint8x16_t vaddq_u8(uint8x16_t a,uint8x16_t b)
// uint16x8_t vaddq_u16(uint16x8_t a,uint16x8_t b)
// uint32x4_t vaddq_u32(uint32x4_t a,uint32x4_t b)
// uint64x2_t vaddq_u64(uint64x2_t a,uint64x2_t b)
// --------------------------------------------------
// float32x2_t vadd_f32(float32x2_t a,float32x2_t b)
// float64x1_t vadd_f64(float64x1_t a,float64x1_t b)
//
// float32x4_t vaddq_f32(float32x4_t a,float32x4_t b)
// float64x2_t vaddq_f64(float64x2_t a,float64x2_t b)
TEST_CASE(test_vadd_s8) {
    struct {
        int8_t a[8];
        int8_t b[8];
        int8_t r[8];
    } test_vec[] = {
        {{-51, 8, -16, -79, -121, -101, -47, 115},
         {-103, 43, -76, -98, -75, 79, -61, 120},
         {102, 51, -92, 79, 60, -22, -108, -21}},
        {{28, -124, -17, -117, -124, -22, 125, 109},
         {14, -125, 88, 115, -23, 119, -31, -73},
         {42, 7, 71, -2, 109, 97, 94, 36}},
        {{INT8_MAX, -47, 104, 6, 109, 57, 121, 6},
         {100, 45, -92, 25, 125, 104, -111, -103},
         {-29, -2, 12, 31, -22, -95, 10, -97}},
        {{-20, INT8_MIN, 37, 112, 106, -94, -35, 120},
         {37, 54, -20, 14, -83, -51, -59, 44},
         {17, -74, 17, 126, 23, 111, -94, -92}},
        {{-98, 45, 51, 11, 102, -84, 17, -53},
         {-38, -74, -28, 87, 30, 118, -16, 10},
         {120, -29, 23, 98, -124, 34, 1, -43}},
        {{-10, 21, 122, 97, -73, 88, -39, -36},
         {-114, -59, -21, 59, -110, -80, 103, 49},
         {-124, -38, 101, -100, 73, 8, 64, 13}},
        {{-34, -102, 60, 68, 71, 78, 15, 33},
         {4, -12, 120, 34, 106, 104, 44, 96},
         {-30, -114, -76, 102, -79, -74, 59, -127}},
        {{126, -90, -63, 53, -2, -101, 18, -116},
         {96, -3, -57, -13, -83, 47, 36, -117},
         {-34, -93, -120, 40, -85, -54, 54, 23}}};

    for (size_t i = 0; i < (sizeof(test_vec) / sizeof(test_vec[0])); i++) {
        int8x8_t a = vld1_s8(test_vec[i].a);
        int8x8_t b = vld1_s8(test_vec[i].b);
        int8x8_t r = vadd_s8(a, b);
        int8x8_t check = vld1_s8(test_vec[i].r);
        ASSERT_EQUAL(check, r);
    }

    return 0;
}

TEST_CASE(test_vaddq_s8) {
    struct {
        int8_t a[16];
        int8_t b[16];
        int8_t r[16];
    } test_vec[] = {
        {{111, -97, -90, 69, 113, 56, -115, 112, 53, -107, -44, 82, -51, -96,
          -21, -97},
         {-99, 22, 102, INT8_MIN, 111, 102, 52, -31, 89, 28, -108, -49, 112,
          -116, 125, -33},
         {12, -75, 12, -59, -32, -98, -63, 81, -114, -79, 104, 33, 61, 44, 104,
          126}},
        {{43, 35, 36, -100, 92, -78, 12, -111, 71, -32, -28, 20, -127, -49, -77,
          30},
         {-26, 25, -97, 85, INT8_MIN, -45, 54, -39, -17, -54, -88, 95, 86, 37,
          63, -126},
         {17, 60, -61, -15, -36, -123, 66, 106, 54, -86, -116, 115, -41, -12,
          -14, -96}},
        {{73, 99, 30, -91, 21, 43, 54, 92, 11, 26, 112, -116, -22, 35, -85,
          -48},
         {61, 74, 37, -67, 29, 91, -106, 12, 37, 62, 108, 124, 99, -85, -2,
          -84},
         {-122, -83, 67, 98, 50, -122, -52, 104, 48, 88, -36, 8, 77, -50, -87,
          124}},
        {{14, 28, 81, 36, 71, -120, INT8_MIN, 83, -94, -15, -33, -116, 20, -118,
          92, 81},
         {-44, -127, 14, -15, -36, -92, -2, 2, -30, 106, 126, 70, 21, 124, -14,
          35},
         {-30, -99, 95, 21, 35, 44, 126, 85, -124, 91, 93, -46, 41, 6, 78,
          116}},
        {{-104, 68, 71, -32, -52, -56, 51, 110, -71, 18, -5, -51, -99, 87, 31,
          113},
         {-39, 45, 99, -75, -46, 97, -73, -76, -53, 53, -6, -32, -79, -19, 3,
          74},
         {113, 113, -86, -107, -98, 41, -22, 34, -124, 71, -11, -83, 78, 68, 34,
          -69}},
        {{49, 75, 42, -3, 19, 93, 107, -52, 111, 102, -103, 12, -66, -72, 126,
          -105},
         {-26, -31, 76, -72, 66, 4, 108, 13, 57, 103, -19, -21, 84, -16, 53,
          -123},
         {23, 44, 118, -75, 85, 97, -41, -39, -88, -51, -122, -9, 18, -88, -77,
          28}},
        {{59, 95, -126, 78, -68, -19, 26, 43, 84, -76, 56, 18, 108, -74, -87,
          82},
         {-105, -11, 10, -39, -7, 119, -26, 51, -34, -45, 30, 50, -61, 83, -73,
          -1},
         {-46, 84, -116, 39, -75, 100, 0, 94, 50, -121, 86, 68, 47, 9, 96, 81}},
        {{-78, 57, 77, 110, 38, 104, -103, 122, 28, -47, -116, -120, -121, 53,
          -37, 30},
         {43, -27, -9, 36, 92, -35, 87, 58, -80, 117, 108, 116, -56, 35, 115,
          122},
         {-35, 30, 68, -110, -126, 69, -16, -76, -52, 70, -8, -4, 79, 88, 78,
          -104}}};

    for (size_t i = 0; i < (sizeof(test_vec) / sizeof(test_vec[0])); i++) {
        int8x16_t a = vld1q_s8(test_vec[i].a);
        int8x16_t b = vld1q_s8(test_vec[i].b);
        int8x16_t r = vaddq_s8(a, b);
        int8x16_t check = vld1q_s8(test_vec[i].r);
        ASSERT_EQUAL(r, check);
    }
    return 0;
}
