// 2023-04-14 19:36
#include <neon.h>
#include <neon_test.h>
// int8x8_t vmul_s8(int8x8_t a,int8x8_t b)
// int16x4_t vmul_s16(int16x4_t a,int16x4_t b)
// int32x2_t vmul_s32(int32x2_t a,int32x2_t b)
// uint8x8_t vmul_u8(uint8x8_t a,uint8x8_t b)
// uint16x4_t vmul_u16(uint16x4_t a,uint16x4_t b)
// uint32x2_t vmul_u32(uint32x2_t a,uint32x2_t b)
//
// int16x8_t vmulq_s16(int16x8_t a,int16x8_t b)
//               ^--- 128-bit vector
// int8x16_t vmulq_s8(int8x16_t a,int8x16_t b)
// int32x4_t vmulq_s32(int32x4_t a,int32x4_t b)
// uint8x16_t vmulq_u8(uint8x16_t a,uint8x16_t b)
// uint16x8_t vmulq_u16(uint16x8_t a,uint16x8_t b)
// uint32x4_t vmulq_u32(uint32x4_t a,uint32x4_t b)
// --------------------------------------------------
// float32x2_t vmul_f32(float32x2_t a,float32x2_t b)
// float64x1_t vmul_f64(float64x1_t a,float64x1_t b)
//
// float64x2_t vmulq_f64(float64x2_t a,float64x2_t b)
// float32x4_t vmulq_f32(float32x4_t a,float32x4_t b)
//
TEST_CASE(test_vmul_s8) {
    struct {
        int8_t a[8];
        int8_t b[8];
        int8_t r[8];
    } test_vec[] = {
        {{106, 59, 38, 92, 101, -69, 33, -125},
         {116, 9, 111, -27, -82, -53, -72, -89},
         {8, 19, 122, 76, -90, 73, -72, 117}},
        {{-49, 114, -110, -77, 44, -114, -19, -70},
         {-124, 116, -15, -94, 118, 102, 35, -31},
         {-68, -88, 114, 70, 72, -108, 103, 122}},
        {{-95, 73, 61, 7, 4, 94, -118, 120},
         {104, -6, 93, 22, -59, 21, -67, -108},
         {104, 74, 41, -102, 20, -74, -30, 96}},
        {{-121, 80, 71, -76, -34, 52, 110, 99},
         {-88, 96, 5, 31, -58, 40, 0, 104},
         {-104, 0, 99, -52, -76, 32, 0, 56}},
        {{113, 61, 111, 117, -101, -7, -19, 3},
         {-13, 75, 25, -72, 96, -41, 76, -24},
         {67, -33, -41, 24, 32, 31, 92, -72}},
        {{39, -109, -100, 5, -56, 10, 104, 112},
         {106, 109, -113, 49, -107, -113, -103, 6},
         {38, -105, 36, -11, 104, -106, 40, -96}},
        {{-52, 8, 123, 104, 1, 105, 107, -11},
         {-76, -123, -83, 20, 92, -6, -4, -125},
         {112, 40, 31, 32, 92, -118, 84, 95}},
        {{-115, -104, -120, 85, -93, -15, -58, 13},
         {94, 85, 62, -12, -27, -41, -6, -79},
         {-58, 120, -16, 4, -49, 103, 92, -3}},
    };

    for (size_t i = 0; i < (sizeof(test_vec) / sizeof(test_vec[0])); i++) {
        int8x8_t a = vld1_s8(test_vec[i].a);
        int8x8_t b = vld1_s8(test_vec[i].b);
        int8x8_t r = vmul_s8(a, b);
        int8x8_t check = vld1_s8(test_vec[i].r);
        ASSERT_EQUAL(r, check);
    }
    return 0;
}

TEST_CASE(test_vmulq_s8) {
    struct {
        int8_t a[16];
        int8_t b[16];
        int8_t r[16];
    } test_vec[] = {
        {{-74, 14, 98, -34, 126, 71, -106, -123, -76, 59, -35, 50, 118, -17, 52,
          99},
         {90, 106, -48, -89, 6, -49, 117, -108, -71, 79, 109, -80, -40, 27, 105,
          -114},
         {-4, -52, -96, -46, -12, 105, -114, -28, 20, 53, 25, 96, -112, 53, 84,
          -22}},
        {{41, -53, 108, -89, 18, 3, 44, -58, 62, 10, -8, -76, -7, 44, 23, 84},
         {-106, -25, -5, -100, -74, 112, 48, 111, -64, -99, 32, -104, -72, -119,
          38, -31},
         {6, 45, -28, -60, -52, 80, 64, -38, INT8_MIN, 34, 0, -32, -8, -116,
          106, -44}},
        {{84, -109, -120, 102, -106, -76, 44, -44, -66, 36, -120, -72, 80, -97,
          12, -26},
         {-122, 7, -126, 61, 119, -78, -84, 55, 79, -52, -49, 7, 85, -10, -24,
          -87},
         {-8, 5, 16, 78, -70, 40, -112, -116, -94, -80, -8, 8, -112, -54, -32,
          -42}},
        {{-119, 112, 15, 31, 36, 59, -13, -29, 95, 123, -101, -81, 26, -89,
          -107, -96},
         {-82, 23, -35, 37, -55, -118, 93, 24, 86, 44, 31, -84, 34, 7, 85, -85},
         {30, 16, -13, 123, 68, -50, 71, 72, -22, 36, -59, -108, 116, -111, 121,
          -32}},
        {{119, 101, -54, -100, -96, -67, INT8_MAX, 0, 56, 26, -81, 82, -63, 69,
          -13, 111},
         {92, -48, -108, 38, 90, -15, 62, -79, 30, 94, 93, 64, 101, -78, -20,
          -35},
         {-60, 16, -56, 40, 64, -19, -62, 0, -112, -116, -109, INT8_MIN, 37, -6,
          4, -45}},
        {{23, -74, 121, -72, 116, -8, -72, -84, 18, 103, -1, -45, -84, -14, 66,
          9},
         {-62, -42, 47, 29, -56, 109, -50, -26, -53, 43, 38, 49, -35, 18, 14,
          -11},
         {110, 36, 55, -40, -96, -104, 16, -120, 70, 77, -38, 99, 124, 4, -100,
          -99}},
        {{-55, -121, -83, 61, INT8_MAX, 101, -23, -111, -52, -24, 100, 121, -38,
          -90, -126, -99},
         {124, -79, -70, 68, 30, -120, 42, -22, -77, 81, 27, -112, 99, 41, -123,
          44},
         {92, 87, -78, 52, -30, -88, 58, -118, -92, 104, -116, 16, 78, -106,
          -118, -4}},
        {{-80, 50, 105, 47, -105, 83, -64, 100, 59, 36, -35, 22, -54, 95, -77,
          70},
         {16, 109, -117, 46, -11, -75, 24, -88, 6, 51, 56, 106, 92, -66, -106,
          12},
         {0, 74, 3, 114, -125, -81, 0, -96, 98, 44, 88, 28, -104, -126, -30,
          72}},
    };

    for (size_t i = 0; i < (sizeof(test_vec) / sizeof(test_vec[0])); i++) {
        int8x16_t a = vld1q_s8(test_vec[i].a);
        int8x16_t b = vld1q_s8(test_vec[i].b);
        int8x16_t r = vmulq_s8(a, b);
        int8x16_t check = vld1q_s8(test_vec[i].r);
        ASSERT_EQUAL(r, check);
    }
    return 0;
}
